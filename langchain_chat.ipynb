{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7335740,
          "sourceType": "datasetVersion",
          "datasetId": 4258183
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyren123/LLM/blob/main/langchain_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import subprocess\n",
        "import shlex\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-04T10:47:06.647617Z",
          "iopub.execute_input": "2024-01-04T10:47:06.648427Z",
          "iopub.status.idle": "2024-01-04T10:47:07.124220Z",
          "shell.execute_reply.started": "2024-01-04T10:47:06.648387Z",
          "shell.execute_reply": "2024-01-04T10:47:07.123005Z"
        },
        "trusted": true,
        "id": "rdO7cwjFG5Tc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmKmcC-sHBPt",
        "outputId": "783ac1e5-b1fb-4e75-b4dd-64918153bffd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/chatchat-space/Langchain-Chatchat.git"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T10:47:14.579274Z",
          "iopub.execute_input": "2024-01-04T10:47:14.579942Z",
          "iopub.status.idle": "2024-01-04T10:47:21.315045Z",
          "shell.execute_reply.started": "2024-01-04T10:47:14.579877Z",
          "shell.execute_reply": "2024-01-04T10:47:21.313880Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et12p3afG5Ti",
        "outputId": "94e94273-f727-4b2e-fbe6-f0607f2c88c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Langchain-Chatchat'...\n",
            "remote: Enumerating objects: 9016, done.\u001b[K\n",
            "remote: Counting objects: 100% (328/328), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 9016 (delta 190), reused 225 (delta 127), pack-reused 8688\u001b[K\n",
            "Receiving objects: 100% (9016/9016), 69.45 MiB | 31.36 MiB/s, done.\n",
            "Resolving deltas: 100% (5451/5451), done.\n",
            "Submodule 'knowledge_base/samples/content/wiki' (https://github.com/chatchat-space/Langchain-Chatchat.wiki.git) registered for path 'knowledge_base/samples/content/wiki'\n",
            "Cloning into '/content/Langchain-Chatchat/knowledge_base/samples/content/wiki'...\n",
            "remote: Enumerating objects: 565, done.        \n",
            "remote: Total 565 (delta 0), reused 0 (delta 0), pack-reused 565        \n",
            "Receiving objects: 100% (565/565), 117.39 KiB | 5.33 MiB/s, done.\n",
            "Resolving deltas: 100% (354/354), done.\n",
            "Submodule path 'knowledge_base/samples/content/wiki': checked out '2f24adb218f23eab00d7fcd7ccf5072f2f35cb3c'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Langchain-Chatchat\n",
        "!pwd\n",
        "! git lfs install\n",
        "! git clone https://huggingface.co/zhr6212/GLM_sft_Lora\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T10:47:25.392042Z",
          "iopub.execute_input": "2024-01-04T10:47:25.392464Z",
          "iopub.status.idle": "2024-01-04T10:50:50.630554Z",
          "shell.execute_reply.started": "2024-01-04T10:47:25.392427Z",
          "shell.execute_reply": "2024-01-04T10:50:50.629397Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boOSYwNAG5Tj",
        "outputId": "f0f084b3-fc8f-4f77-972a-41197d016e26"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Langchain-Chatchat\n",
            "/content/Langchain-Chatchat\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'GLM_sft_Lora'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (20/20), 26.71 KiB | 3.82 MiB/s, done.\n",
            "Filtering content: 100% (6/6), 11.63 GiB | 47.74 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/^torch/# &/' requirements.txt\n",
        "!sed -i 's/^torchvision/# &/' requirements.txt\n",
        "!sed -i 's/^torchaudio/# &/' requirements.txt\n",
        "!pip3 install -r requirements.txt\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:53:52.440369Z",
          "iopub.execute_input": "2024-01-04T07:53:52.441145Z",
          "iopub.status.idle": "2024-01-04T07:55:04.707792Z",
          "shell.execute_reply.started": "2024-01-04T07:53:52.441086Z",
          "shell.execute_reply": "2024-01-04T07:55:04.706678Z"
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "Lmpv3SuvG5Tj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's#\\<THUDM/chatglm3-6b\\>#/content/Langchain-Chatchat/GLM_sft_Lora#' /content/Langchain-Chatchat/configs/model_config.py.example\n",
        "!sed -i 's#\\<chatglm3-6b\\>#GLM_sft_Lora#g' /content/Langchain-Chatchat/configs/model_config.py.example"
      ],
      "metadata": {
        "id": "XcarAkMimAzy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Langchain-Chatchat/knowledge_base/samples/content/llm\n",
        "!rm -rf /content/Langchain-Chatchat/knowledge_base/samples/content/test_files/*\n",
        "!rm -rf /content/Langchain-Chatchat/knowledge_base/samples/content/wiki/*"
      ],
      "metadata": {
        "id": "u2JSd1jhdSji"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:55:04.709085Z",
          "iopub.execute_input": "2024-01-04T07:55:04.709374Z",
          "iopub.status.idle": "2024-01-04T07:55:04.714213Z",
          "shell.execute_reply.started": "2024-01-04T07:55:04.709348Z",
          "shell.execute_reply": "2024-01-04T07:55:04.713313Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtJqY5yWG5Tk",
        "outputId": "80b43797-11a1-4e34-ec82-db263a444678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "L3n_0zM4K1hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 如果有GPU可用，且PyTorch与CUDA兼容，将输出CUDA设备信息\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:55:04.715351Z",
          "iopub.execute_input": "2024-01-04T07:55:04.715702Z",
          "iopub.status.idle": "2024-01-04T07:55:07.095665Z",
          "shell.execute_reply.started": "2024-01-04T07:55:04.715674Z",
          "shell.execute_reply": "2024-01-04T07:55:07.094249Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWEH3hFNG5Tk",
        "outputId": "67d61ca5-8b60-4857-e7bb-ee6382e47019"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.version.cuda)\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torchvision\n",
        "print(torchvision.__version__)\n",
        "import torchaudio\n",
        "print(torchaudio.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:55:07.098064Z",
          "iopub.status.idle": "2024-01-04T07:55:07.098384Z",
          "shell.execute_reply.started": "2024-01-04T07:55:07.098232Z",
          "shell.execute_reply": "2024-01-04T07:55:07.098247Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN5g8n4UG5Tl",
        "outputId": "3f0f0605-4f53-4d29-8978-23dec5ad2c5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.1\n",
            "2.1.0+cu121\n",
            "0.16.0+cu121\n",
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install paddlepaddle-gpu==2.6.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:55:07.100038Z",
          "iopub.status.idle": "2024-01-04T07:55:07.100487Z",
          "shell.execute_reply.started": "2024-01-04T07:55:07.100265Z",
          "shell.execute_reply": "2024-01-04T07:55:07.100286Z"
        },
        "trusted": true,
        "id": "MGP_s_OmG5Tm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e93042-7533-45d6-e8b7-cd5585300b6a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting paddlepaddle-gpu==2.6.0\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6b/9b/634ab295b004116c7717e226dbb1b52bd9c90b87b7acbf3470e8224c5815/paddlepaddle_gpu-2.6.0-cp310-cp310-manylinux1_x86_64.whl (749.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.8/749.8 MB\u001b[0m \u001b[31m941.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu==2.6.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu==2.6.0) (1.24.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu==2.6.0) (9.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu==2.6.0) (4.4.2)\n",
            "Collecting astor (from paddlepaddle-gpu==2.6.0)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu==2.6.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu==2.6.0) (3.20.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu==2.6.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu==2.6.0) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu==2.6.0) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu==2.6.0) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->paddlepaddle-gpu==2.6.0) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->paddlepaddle-gpu==2.6.0) (1.2.0)\n",
            "Installing collected packages: astor, paddlepaddle-gpu\n",
            "Successfully installed astor-0.8.1 paddlepaddle-gpu-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
        "# !curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
        "!ngrok config add-authtoken 2YgfRwaI3GHwnY58VdLGAYWndn5_2LfP9coSjisqWvz5AbX7h\n",
        "ngrok_command='ngrok http 8501 --domain=terrier-accurate-eagerly.ngrok-free.app '"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:55:07.102023Z",
          "iopub.status.idle": "2024-01-04T07:55:07.102385Z",
          "shell.execute_reply.started": "2024-01-04T07:55:07.102218Z",
          "shell.execute_reply": "2024-01-04T07:55:07.102235Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y8O52vVG5Tm",
        "outputId": "8755aa8b-f4fa-4bed-ad47-11bf726bf746"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [3,407 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,332 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,307 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,047 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,582 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,611 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,606 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,257 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,158 kB]\n",
            "Fetched 12.2 MB in 2s (5,036 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "27 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 6,542 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.5.0 [6,542 kB]\n",
            "Fetched 6,542 kB in 1s (10.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 121654 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/ngrok_3.5.0_amd64.deb ...\n",
            "Unpacking ngrok (3.5.0) ...\n",
            "Setting up ngrok (3.5.0) ...\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python copy_config_example.py\n",
        "! conda install faiss-gpu -c conda-forge\n",
        "# !cp /kaggle/input/chatglm3/model_config.py /kaggle/working/Langchain-Chatchat/configs/\n",
        "# !cp /kaggle/input/chatglm3/server_config.py /kaggle/working/Langchain-Chatchat/configs/\n",
        "! python copy_config_example.py\n",
        "! python init_database.py --recreate-vs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:55:07.104449Z",
          "iopub.status.idle": "2024-01-04T07:55:07.105156Z",
          "shell.execute_reply.started": "2024-01-04T07:55:07.104885Z",
          "shell.execute_reply": "2024-01-04T07:55:07.104909Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F9f2tiUG5Tm",
        "outputId": "38b002dd-cbf7-450f-c7e9-9a0123fec207"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "recreating all vector stores\n",
            "2024-01-05 12:10:00,626 - faiss_cache.py[line:94] - INFO: loading vector store in 'samples/vector_store/bge-large-zh' from disk.\n",
            "2024-01-05 12:10:02,051 - SentenceTransformer.py[line:66] - INFO: Load pretrained SentenceTransformer: BAAI/bge-large-zh\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 5.93MB/s]\n",
            "1_Pooling/config.json: 100% 191/191 [00:00<00:00, 820kB/s]\n",
            "README.md: 100% 27.9k/27.9k [00:00<00:00, 62.0MB/s]\n",
            "config.json: 100% 941/941 [00:00<00:00, 3.65MB/s]\n",
            "config_sentence_transformers.json: 100% 124/124 [00:00<00:00, 446kB/s]\n",
            "model.safetensors: 100% 1.30G/1.30G [00:10<00:00, 120MB/s]\n",
            "pytorch_model.bin: 100% 1.30G/1.30G [00:07<00:00, 170MB/s]\n",
            "sentence_bert_config.json: 100% 52.0/52.0 [00:00<00:00, 278kB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 554kB/s]\n",
            "tokenizer.json: 100% 439k/439k [00:00<00:00, 3.34MB/s]\n",
            "tokenizer_config.json: 100% 366/366 [00:00<00:00, 1.84MB/s]\n",
            "vocab.txt: 100% 110k/110k [00:00<00:00, 141MB/s]\n",
            "modules.json: 100% 349/349 [00:00<00:00, 1.63MB/s]\n",
            "Batches: 100% 1/1 [00:00<00:00,  1.00it/s]\n",
            "2024-01-05 12:10:31,340 - loader.py[line:54] - INFO: Loading faiss with AVX2 support.\n",
            "2024-01-05 12:10:31,422 - loader.py[line:56] - INFO: Successfully loaded faiss with AVX2 support.\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/faiss.py:121: UserWarning: Normalizing L2 is not applicable for metric type: METRIC_INNER_PRODUCT\n",
            "  warnings.warn(\n",
            "2024-01-05 12:10:31,471 - faiss_cache.py[line:94] - INFO: loading vector store in 'samples/vector_store/bge-large-zh' from disk.\n",
            "Batches: 100% 1/1 [00:00<00:00, 53.89it/s]\n",
            "2024-01-05 12:10:31,499 - utils.py[line:289] - INFO: UnstructuredMarkdownLoader used for /content/Langchain-Chatchat/knowledge_base/samples/content/llm/分布式训练技术原理.md\n",
            "2024-01-05 12:10:31,500 - utils.py[line:289] - INFO: UnstructuredMarkdownLoader used for /content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型技术栈-算法与原理.md\n",
            "2024-01-05 12:10:31,500 - utils.py[line:289] - INFO: UnstructuredMarkdownLoader used for /content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型指令对齐训练原理.md\n",
            "2024-01-05 12:10:31,500 - utils.py[line:289] - INFO: UnstructuredMarkdownLoader used for /content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型应用技术原理.md\n",
            "2024-01-05 12:10:31,501 - utils.py[line:289] - INFO: UnstructuredMarkdownLoader used for /content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型技术栈-实战与应用.md\n",
            "2024-01-05 12:10:31,501 - utils.py[line:289] - INFO: UnstructuredMarkdownLoader used for /content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型推理优化策略.md\n",
            "2024-01-05 12:10:34,205 - xml.py[line:105] - INFO: Reading document from string ...\n",
            "2024-01-05 12:10:34,219 - xml.py[line:105] - INFO: Reading document from string ...\n",
            "2024-01-05 12:10:34,240 - html.py[line:151] - INFO: Reading document ...\n",
            "2024-01-05 12:10:34,242 - common.py[line:601] - INFO: HTML element instance has no attribute type\n",
            "2024-01-05 12:10:34,241 - html.py[line:151] - INFO: Reading document ...\n",
            "2024-01-05 12:10:34,285 - common.py[line:601] - INFO: HTML element instance has no attribute type\n",
            "2024-01-05 12:10:34,270 - xml.py[line:105] - INFO: Reading document from string ...\n",
            "2024-01-05 12:10:34,291 - html.py[line:151] - INFO: Reading document ...\n",
            "2024-01-05 12:10:34,293 - common.py[line:601] - INFO: HTML element instance has no attribute type\n",
            "2024-01-05 12:10:34,265 - xml.py[line:105] - INFO: Reading document from string ...\n",
            "2024-01-05 12:10:34,293 - html.py[line:151] - INFO: Reading document ...\n",
            "2024-01-05 12:10:34,295 - common.py[line:601] - INFO: HTML element instance has no attribute type\n",
            "2024-01-05 12:10:34,256 - xml.py[line:105] - INFO: Reading document from string ...\n",
            "2024-01-05 12:10:34,312 - html.py[line:151] - INFO: Reading document ...\n",
            "2024-01-05 12:10:34,313 - common.py[line:601] - INFO: HTML element instance has no attribute type\n",
            "2024-01-05 12:10:34,466 - xml.py[line:105] - INFO: Reading document from string ...\n",
            "2024-01-05 12:10:34,472 - html.py[line:151] - INFO: Reading document ...\n",
            "2024-01-05 12:10:34,487 - common.py[line:601] - INFO: HTML element instance has no attribute type\n",
            "2024-01-05 12:10:34,573 - tokenization_chatglm.py[line:164] - WARNING: Setting eos_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,573 - tokenization_chatglm.py[line:160] - WARNING: Setting pad_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,573 - tokenization_chatglm.py[line:156] - WARNING: Setting unk_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,666 - tokenization_chatglm.py[line:164] - WARNING: Setting eos_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,749 - tokenization_chatglm.py[line:160] - WARNING: Setting pad_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,822 - tokenization_chatglm.py[line:156] - WARNING: Setting unk_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,801 - tokenization_chatglm.py[line:164] - WARNING: Setting eos_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,834 - tokenization_chatglm.py[line:160] - WARNING: Setting pad_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,834 - tokenization_chatglm.py[line:156] - WARNING: Setting unk_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,801 - tokenization_chatglm.py[line:164] - WARNING: Setting eos_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,839 - tokenization_chatglm.py[line:160] - WARNING: Setting pad_token is not supported, use the default one.\n",
            "2024-01-05 12:10:34,839 - tokenization_chatglm.py[line:156] - WARNING: Setting unk_token is not supported, use the default one.\n",
            "文档切分示例：page_content='大模型应用技术原理\\nRAG\\n向量数据库 对比\\n选型标准\\n开源vs.闭源vs. 源码可见\\n客户端/SDK语言\\n托管方式\\nself-hosted/on-premise\\nredis,pgvector,milvus\\nmanaged/cloud-native\\nzilliz,pinecone\\nembeded+cloud-native\\nchroma,lanceDB\\nself-hosted+cloud-native\\nvald,drant,weaviate,vspa,elasticsearch\\n索引方法\\n算法\\nFlat\\nTree-based\\nAnnoy(Approximate Nearest Neighbors Oh Yeah)\\nKD-Tree\\nTrinary Projection Trees\\nIVF\\nIVF\\nIVMF(Inverted Multi-index File)\\nGraph-based\\nHNSW\\nNSG\\nVamana(DiskANN)' metadata={'source': '/content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型应用技术原理.md'}\n",
            "正在将 samples/llm/大模型应用技术原理.md 添加到向量库，共包含7条文档\n",
            "文档切分示例：page_content='大模型技术栈-实战与应用\\n训练框架\\ndeepspeed\\nmegatron-lm\\ncolossal-ai\\ntrlx\\n推理框架\\ntriton\\nvllm\\ntext-generation-inference\\nlit-llama\\nlightllm\\nTensorRT-LLM(原FasterTransformer)\\nfastllm\\ninferllm\\nllama-cpp\\nopenPPL-LLM\\n压缩框架\\nbitsandbytes\\nauto-gptq\\ndeepspeed\\nembedding框架\\nsentence-transformer\\nFlagEmbedding\\n向量数据库 向量数据库对比\\nfaiss\\npgvector\\nmilvus\\npinecone\\nweaviate\\nLanceDB\\nChroma\\n应用框架\\nAuto-GPT\\nlangchain\\nllama-index\\nquivr\\npython前端\\nstreamlit\\ngradio' metadata={'source': '/content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型技术栈-实战与应用.md'}\n",
            "文档切分示例：page_content='分布式训练技术原理\\n数据并行\\nFSDP\\nFSDP算法是由来自DeepSpeed的ZeroRedundancyOptimizer技术驱动的，但经过修改的设计和实现与PyTorch的其他组件保持一致。FSDP将模型实例分解为更小的单元，然后将每个单元内的所有参数扁平化和分片。分片参数在计算前按需通信和恢复，计算结束后立即丢弃。这种方法确保FSDP每次只需要实现一个单元的参数，这大大降低了峰值内存消耗。(数据并行+Parameter切分)\\nDDP\\nDistributedDataParallel (DDP)， 在每个设备上维护一个模型副本，并通过向后传递的集体AllReduce操作同步梯度，从而确保在训练期间跨副本的模型一致性 。为了加快训练速度， DDP将梯度通信与向后计算重叠 ，促进在不同资源上并发执行工作负载。' metadata={'source': '/content/Langchain-Chatchat/knowledge_base/samples/content/llm/分布式训练技术原理.md'}\n",
            "文档切分示例：page_content='大模型指令对齐训练原理\\nRLHF\\nSFT\\nRM\\nPPO\\nAIHF-based\\nRLAIF\\n核心在于通过AI 模型监督其他 AI 模型，即在SFT阶段，从初始模型中采样，然后生成自我批评和修正，然后根据修正后的反应微调原始模型。在 RL 阶段，从微调模型中采样，使用一个模型来评估生成的样本，并从这个 AI 偏好数据集训练一个偏好模型。然后使用偏好模型作为奖励信号对 RL 进行训练' metadata={'source': '/content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型指令对齐训练原理.md'}\n",
            "2024-01-05 12:10:35,708 - tokenization_chatglm.py[line:164] - WARNING: Setting eos_token is not supported, use the default one.\n",
            "2024-01-05 12:10:35,708 - tokenization_chatglm.py[line:160] - WARNING: Setting pad_token is not supported, use the default one.\n",
            "2024-01-05 12:10:35,708 - tokenization_chatglm.py[line:156] - WARNING: Setting unk_token is not supported, use the default one.\n",
            "Batches:   0% 0/1 [00:00<?, ?it/s]文档切分示例：page_content='大模型技术栈-算法与原理\\ntokenizer方法\\nword-level\\nchar-level\\nsubword-level\\nBPE\\nWordPiece\\nUniLM\\nSentencePiece\\nByteBPE\\nposition encoding\\n绝对位置编码\\nROPE\\nAliBi\\n相对位置编码\\nTransformer-XL\\nT5/TUPE\\nDeBERTa\\n其他位置编码\\n注意力机制\\n稀疏注意力\\nflash-attention' metadata={'source': '/content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型技术栈-算法与原理.md'}\n",
            "2024-01-05 12:10:36,322 - tokenization_chatglm.py[line:164] - WARNING: Setting eos_token is not supported, use the default one.\n",
            "2024-01-05 12:10:36,323 - tokenization_chatglm.py[line:160] - WARNING: Setting pad_token is not supported, use the default one.\n",
            "2024-01-05 12:10:36,323 - tokenization_chatglm.py[line:156] - WARNING: Setting unk_token is not supported, use the default one.\n",
            "Batches: 100% 1/1 [00:00<00:00,  2.03it/s]\n",
            "文档切分示例：page_content='大模型推理优化策略' metadata={'source': '/content/Langchain-Chatchat/knowledge_base/samples/content/llm/大模型推理优化策略.md'}\n",
            "正在将 samples/llm/大模型技术栈-实战与应用.md 添加到向量库，共包含2条文档\n",
            "Batches: 100% 1/1 [00:00<00:00, 15.35it/s]\n",
            "正在将 samples/llm/分布式训练技术原理.md 添加到向量库，共包含11条文档\n",
            "Batches: 100% 1/1 [00:00<00:00,  1.61it/s]\n",
            "正在将 samples/llm/大模型指令对齐训练原理.md 添加到向量库，共包含4条文档\n",
            "Batches: 100% 1/1 [00:00<00:00,  4.09it/s]\n",
            "正在将 samples/llm/大模型技术栈-算法与原理.md 添加到向量库，共包含30条文档\n",
            "Batches: 100% 1/1 [00:01<00:00,  1.93s/it]\n",
            "正在将 samples/llm/大模型推理优化策略.md 添加到向量库，共包含10条文档\n",
            "Batches: 100% 1/1 [00:00<00:00,  1.62it/s]\n",
            "2024-01-05 12:10:40,256 - faiss_cache.py[line:38] - INFO: 已将向量库 ('samples', 'bge-large-zh') 保存到磁盘\n",
            "总计用时： 0:00:39.767839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_process = subprocess.Popen(shlex.split(ngrok_command), stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
        "! python startup.py -a"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-04T07:55:07.106527Z",
          "iopub.status.idle": "2024-01-04T07:55:07.106954Z",
          "shell.execute_reply.started": "2024-01-04T07:55:07.106732Z",
          "shell.execute_reply": "2024-01-04T07:55:07.106752Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv7miIkOG5Tn",
        "outputId": "ee0ecd6b-2731-4dce-c855-df52d3387a22"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==============================Langchain-Chatchat Configuration==============================\n",
            "操作系统：Linux-6.1.58+-x86_64-with-glibc2.35.\n",
            "python版本：3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "项目版本：v0.2.9\n",
            "langchain版本：0.0.352. fastchat版本：0.2.34\n",
            "\n",
            "\n",
            "当前使用的分词器：ChineseRecursiveTextSplitter\n",
            "当前启动的LLM模型：['GLM_sft_Lora', 'zhipu-api', 'openai-api'] @ cuda\n",
            "{'device': 'cuda',\n",
            " 'host': '0.0.0.0',\n",
            " 'infer_turbo': False,\n",
            " 'model_path': '/content/Langchain-Chatchat/GLM_sft_Lora',\n",
            " 'model_path_exists': True,\n",
            " 'port': 20002}\n",
            "{'api_key': '',\n",
            " 'device': 'auto',\n",
            " 'host': '0.0.0.0',\n",
            " 'infer_turbo': False,\n",
            " 'online_api': True,\n",
            " 'port': 21001,\n",
            " 'provider': 'ChatGLMWorker',\n",
            " 'version': 'chatglm_turbo',\n",
            " 'worker_class': <class 'server.model_workers.zhipu.ChatGLMWorker'>}\n",
            "{'api_base_url': 'https://api.openai.com/v1',\n",
            " 'api_key': '',\n",
            " 'device': 'auto',\n",
            " 'host': '0.0.0.0',\n",
            " 'infer_turbo': False,\n",
            " 'model_name': 'gpt-3.5-turbo',\n",
            " 'online_api': True,\n",
            " 'openai_proxy': '',\n",
            " 'port': 20002}\n",
            "当前Embbedings模型： bge-large-zh @ cuda\n",
            "==============================Langchain-Chatchat Configuration==============================\n",
            "\n",
            "\n",
            "2024-01-05 12:11:10,490 - startup.py[line:651] - INFO: 正在启动服务：\n",
            "2024-01-05 12:11:10,490 - startup.py[line:652] - INFO: 如需查看 llm_api 日志，请前往 /content/Langchain-Chatchat/logs\n",
            "2024-01-05 12:11:21 | INFO | model_worker | Register to controller\n",
            "2024-01-05 12:11:21 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8218\u001b[0m]\n",
            "2024-01-05 12:11:21 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "2024-01-05 12:11:21 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "2024-01-05 12:11:21 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:20000\u001b[0m (Press CTRL+C to quit)\n",
            "2024-01-05 12:11:23 | INFO | model_worker | Loading the model ['GLM_sft_Lora'] on worker 959f3f38 ...\n",
            "2024-01-05 12:11:23 | WARNING | transformers_modules.GLM_sft_Lora.tokenization_chatglm | Setting eos_token is not supported, use the default one.\n",
            "2024-01-05 12:11:23 | WARNING | transformers_modules.GLM_sft_Lora.tokenization_chatglm | Setting pad_token is not supported, use the default one.\n",
            "2024-01-05 12:11:23 | WARNING | transformers_modules.GLM_sft_Lora.tokenization_chatglm | Setting unk_token is not supported, use the default one.\n",
            "modeling_chatglm.py:   0% 0.00/55.7k [00:00<?, ?B/s]\n",
            "modeling_chatglm.py: 100% 55.7k/55.7k [00:00<00:00, 993kB/s]\n",
            "2024-01-05 12:11:24 | ERROR | stderr | \n",
            "quantization.py:   0% 0.00/14.7k [00:00<?, ?B/s]\n",
            "quantization.py: 100% 14.7k/14.7k [00:00<00:00, 48.8MB/s]\n",
            "2024-01-05 12:11:24 | ERROR | stderr | \n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
            "- quantization.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "configuration_chatglm.py:   0% 0.00/2.33k [00:00<?, ?B/s]\n",
            "configuration_chatglm.py: 100% 2.33k/2.33k [00:00<00:00, 9.87MB/s]\n",
            "2024-01-05 12:11:24 | ERROR | stderr | \n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
            "- configuration_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
            "- modeling_chatglm.py\n",
            "- quantization.py\n",
            "- configuration_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "Loading checkpoint shards:   0% 0/5 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  20% 1/5 [00:00<00:01,  3.64it/s]\n",
            "Loading checkpoint shards:  40% 2/5 [00:00<00:00,  3.42it/s]\n",
            "Loading checkpoint shards:  60% 3/5 [00:00<00:00,  3.36it/s]\n",
            "Loading checkpoint shards:  80% 4/5 [00:01<00:00,  3.37it/s]\n",
            "Loading checkpoint shards: 100% 5/5 [00:01<00:00,  3.97it/s]\n",
            "Loading checkpoint shards: 100% 5/5 [00:01<00:00,  3.70it/s]\n",
            "2024-01-05 12:11:25 | ERROR | stderr | \n",
            "2024-01-05 12:12:19 | INFO | model_worker | Register to controller\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8531\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:7861\u001b[0m (Press CTRL+C to quit)\n",
            "\n",
            "\n",
            "==============================Langchain-Chatchat Configuration==============================\n",
            "操作系统：Linux-6.1.58+-x86_64-with-glibc2.35.\n",
            "python版本：3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "项目版本：v0.2.9\n",
            "langchain版本：0.0.352. fastchat版本：0.2.34\n",
            "\n",
            "\n",
            "当前使用的分词器：ChineseRecursiveTextSplitter\n",
            "当前启动的LLM模型：['GLM_sft_Lora', 'zhipu-api', 'openai-api'] @ cuda\n",
            "{'device': 'cuda',\n",
            " 'host': '0.0.0.0',\n",
            " 'infer_turbo': False,\n",
            " 'model_path': '/content/Langchain-Chatchat/GLM_sft_Lora',\n",
            " 'model_path_exists': True,\n",
            " 'port': 20002}\n",
            "{'api_key': '',\n",
            " 'device': 'auto',\n",
            " 'host': '0.0.0.0',\n",
            " 'infer_turbo': False,\n",
            " 'online_api': True,\n",
            " 'port': 21001,\n",
            " 'provider': 'ChatGLMWorker',\n",
            " 'version': 'chatglm_turbo',\n",
            " 'worker_class': <class 'server.model_workers.zhipu.ChatGLMWorker'>}\n",
            "{'api_base_url': 'https://api.openai.com/v1',\n",
            " 'api_key': '',\n",
            " 'device': 'auto',\n",
            " 'host': '0.0.0.0',\n",
            " 'infer_turbo': False,\n",
            " 'model_name': 'gpt-3.5-turbo',\n",
            " 'online_api': True,\n",
            " 'openai_proxy': '',\n",
            " 'port': 20002}\n",
            "当前Embbedings模型： bge-large-zh @ cuda\n",
            "\n",
            "\n",
            "服务端运行信息：\n",
            "    OpenAI API Server: http://127.0.0.1:20000/v1\n",
            "    Chatchat  API  Server: http://127.0.0.1:7861\n",
            "    Chatchat WEBUI Server: http://0.0.0.0:8501\n",
            "==============================Langchain-Chatchat Configuration==============================\n",
            "\n",
            "\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  URL: \u001b[0m\u001b[1mhttp://0.0.0.0:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2024-01-05 12:13:13,999 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:42682 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:14,005 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:14,187 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:42682 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:14,189 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:42682 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:14,200 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:22,425 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:34938 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:22,428 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:22,566 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:34938 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:22,569 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:34938 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:22,583 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:34938 - \"\u001b[1mPOST /chat/chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:22,954 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:23 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:35564 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:23,138 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:23 | INFO | httpx | HTTP Request: POST http://127.0.0.1:20002/worker_generate_stream \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:29.065633: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-05 12:13:29.065753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-05 12:13:29.189671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-05 12:13:31.330264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-05 12:13:37,955 - utils.py[line:177] - ERROR: JSONDecodeError: 接口返回json错误： ‘: ping - 2024-01-05 12:13:37.954496\n",
            "\n",
            "’。错误信息是：Expecting value: line 1 column 1 (char 0)。\n",
            "2024-01-05 12:13:56,928 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:35054 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:56,930 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:57,008 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:35054 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:57,010 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:35054 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:57,019 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:35054 - \"\u001b[1mPOST /chat/chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:57,215 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:57 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:52098 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:13:57,316 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:13:57 | INFO | httpx | HTTP Request: POST http://127.0.0.1:20002/worker_generate_stream \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:08,238 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57466 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:08,240 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:08,320 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57466 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:08,322 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57466 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:08,336 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57466 - \"\u001b[1mPOST /chat/chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:08,536 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:08 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:44632 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:08,635 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:08 | INFO | httpx | HTTP Request: POST http://127.0.0.1:20002/worker_generate_stream \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:19,428 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:44790 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:19,430 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:19,510 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:44790 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:19,512 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:44790 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:19,521 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:44790 - \"\u001b[1mPOST /chat/chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:19,724 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:19 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:56796 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:19,832 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:19 | INFO | httpx | HTTP Request: POST http://127.0.0.1:20002/worker_generate_stream \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:56,543 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:58164 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:56,546 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:56,623 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:58164 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:56,625 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:58164 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:56,633 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:58164 - \"\u001b[1mPOST /chat/chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:56,852 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:56 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:53962 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:14:56,963 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:14:56 | INFO | httpx | HTTP Request: POST http://127.0.0.1:20002/worker_generate_stream \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:18,237 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57776 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:18,240 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:18,373 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57776 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:18,376 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57776 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:18,391 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:21,199 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57792 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:21,202 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:21,281 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57792 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:21,283 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57792 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:21,292 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:57792 - \"\u001b[1mGET /knowledge_base/list_knowledge_bases HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:21,301 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:28,150 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:56002 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:28,153 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:28,235 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:56002 - \"\u001b[1mPOST /llm_model/list_running_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:28,237 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:56002 - \"\u001b[1mPOST /llm_model/list_config_models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:28,256 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:56002 - \"\u001b[1mGET /knowledge_base/list_knowledge_bases HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:28,263 - _client.py[line:1013] - INFO: HTTP Request: GET http://127.0.0.1:7861/knowledge_base/list_knowledge_bases \"HTTP/1.1 200 OK\"\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:56002 - \"\u001b[1mPOST /chat/knowledge_base_chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:28,504 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/knowledge_base_chat \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:28,506 - SentenceTransformer.py[line:66] - INFO: Load pretrained SentenceTransformer: BAAI/bge-large-zh\n",
            "Batches: 100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "2024-01-05 12:15:37,401 - faiss_cache.py[line:94] - INFO: loading vector store in 'samples/vector_store/bge-large-zh' from disk.\n",
            "2024-01-05 12:15:37,407 - loader.py[line:54] - INFO: Loading faiss with AVX2 support.\n",
            "2024-01-05 12:15:37,489 - loader.py[line:56] - INFO: Successfully loaded faiss with AVX2 support.\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/faiss.py:121: UserWarning: Normalizing L2 is not applicable for metric type: METRIC_INNER_PRODUCT\n",
            "  warnings.warn(\n",
            "2024-01-05 12:15:37 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:41120 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "2024-01-05 12:15:37,637 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:37 | INFO | httpx | HTTP Request: POST http://127.0.0.1:20002/worker_generate_stream \"HTTP/1.1 200 OK\"\n",
            "2024-01-05 12:15:43,504 - utils.py[line:177] - ERROR: JSONDecodeError: 接口返回json错误： ‘: ping - 2024-01-05 12:15:43.503812\n",
            "\n",
            "’。错误信息是：Expecting value: line 1 column 1 (char 0)。\n",
            "2024-01-05 12:15:53,577 - startup.py[line:856] - WARNING: Sending SIGKILL to {'zhipu-api': <Process name='api_worker - zhipu-api (8220)' pid=8220 parent=8119 started daemon>}\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "2024-01-05 12:15:53,577 - startup.py[line:856] - WARNING: Sending SIGKILL to {'GLM_sft_Lora': <Process name='model_worker - GLM_sft_Lora (8219)' pid=8219 parent=8119 started daemon>}\n",
            "2024-01-05 12:15:53,577 - startup.py[line:856] - WARNING: Sending SIGKILL to <Process name='controller (8187)' pid=8187 parent=8119 started daemon>\n",
            "2024-01-05 12:15:53,578 - startup.py[line:856] - WARNING: Sending SIGKILL to <Process name='openai_api (8218)' pid=8218 parent=8119 started daemon>\n",
            "2024-01-05 12:15:53,578 - startup.py[line:856] - WARNING: Sending SIGKILL to <Process name='API Server (8531)' pid=8531 parent=8119 started daemon>\n",
            "2024-01-05 12:15:53,578 - startup.py[line:856] - WARNING: Sending SIGKILL to <Process name='WEBUI Server (8584)' pid=8584 parent=8119 started daemon>\n",
            "2024-01-05 12:15:53,583 - startup.py[line:867] - INFO: Process status: {'zhipu-api': <Process name='api_worker - zhipu-api (8220)' pid=8220 parent=8119 started daemon>}\n",
            "2024-01-05 12:15:53,583 - startup.py[line:867] - INFO: Process status: {'GLM_sft_Lora': <Process name='model_worker - GLM_sft_Lora (8219)' pid=8219 parent=8119 started daemon>}\n",
            "2024-01-05 12:15:53,591 - startup.py[line:867] - INFO: Process status: <Process name='controller (8187)' pid=8187 parent=8119 started daemon>\n",
            "2024-01-05 12:15:53,591 - startup.py[line:867] - INFO: Process status: <Process name='openai_api (8218)' pid=8218 parent=8119 started daemon>\n",
            "2024-01-05 12:15:53,592 - startup.py[line:867] - INFO: Process status: <Process name='API Server (8531)' pid=8531 parent=8119 started daemon>\n",
            "2024-01-05 12:15:53,592 - startup.py[line:867] - INFO: Process status: <Process name='WEBUI Server (8584)' pid=8584 parent=8119 started daemon>\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Langchain-Chatchat/startup.py\", line 883, in <module>\n",
            "    loop.run_until_complete(start_main_server())\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/content/Langchain-Chatchat/startup.py\", line 779, in start_main_server\n",
            "  File \"<string>\", line 2, in get\n",
            "  File \"/usr/lib/python3.10/multiprocessing/managers.py\", line 818, in _callmethod\n",
            "    kind, result = conn.recv()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/content/Langchain-Chatchat/startup.py\", line 609, in f\n",
            "    raise KeyboardInterrupt(f\"{signalname} received\")\n",
            "KeyboardInterrupt: SIGINT received\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}